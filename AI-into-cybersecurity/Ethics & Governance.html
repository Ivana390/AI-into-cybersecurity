<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ethics & Governance</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <nav class="navbar">
       <div class="logo">
           <img src="images/logo.png" alt="AI Logo">
           <span>AI into cybersecurity</span>
       </div>
       <ul class="nav-links">
           <li><a href="index.html">Home</a></li>
           <li><a href="App.html">Applications</a></li>
           <li><a href="Risks.html">Risks</a></li>
           <li><a href="About_us.html">About us</a></li>
           <li><a href="#" class="cta-button">Go to chatGPT</a></li>
       </ul>
   </nav>
   </header>
    <main>
    <h>Ethics & Governance</h>
    <p>
        As AI becomes more deeply integrated into cybersecurity strategies,
        concerns around privacy, fairness, and accountability grow in importance.
        Data used to train AI models may include sensitive personal information,
        and failing to handle it responsibly could violate regulations and erode
        trust. Beyond legal considerations, ethical questions arise when 
        algorithms make security-related decisions—such as whether certain users
        or devices are flagged as threats. If these decisions are based on biased 
        or incomplete data, the outcomes could unfairly impact certain individuals or groups.
    </p>
    <p>
        To address these challenges, organizations should adopt robust governance
        frameworks that set clear guidelines for AI-based cybersecurity tools. 
        This includes defining how data is collected, stored, and processed, as
        well as documenting the reasoning behind critical automated decisions. 
        Regular audits of AI models help detect biases, while transparent reporting
        ensures stakeholders can see how and why specific alerts or actions are
        triggered. Additionally, having designated roles—such as a Chief Ethics 
        Officer or data governance committee—provides structured oversight and accountability.
    </p>
    <p>
        Ultimately, a well-governed AI system is not just about meeting legal
        obligations; it also fosters confidence among employees, partners, and 
        customers. By demonstrating that AI-driven security measures are developed 
        and deployed responsibly, organizations can better safeguard user rights 
        and maintain public trust. Human experts remain pivotal in this 
        process—reviewing AI outputs, correcting potential errors, and ensuring 
        that technology aligns with societal values. When ethics and governance
        are integrated into AI solutions, organizations can innovate in cybersecurity 
        without compromising on transparency or fairness.
    </p>
</main>
<footer>
    <section class="items-wrap">
        <section class="links-wrap">
            <a href=""> <img src="images/git-logo.png"></a>
            <a href=""> <img src="images/linkedin-logo.png"></a>
        </section>
        <section class="links-wrap">
            <h3>TERMS OF USE</h3>
            <h3>PRIVACY POLICY</h3>
            <h3>SITE MAP</h3>
        </section>
        <section class="links-wrap">
            <img src="images/en-logo.png">
            <img src="images/bg-logo.png">
            <img src="images/de-logo.png">
        </section>
    </section>
    <h4>Copyright © 2025 Artificial intelligence (AI) into cybersecurity. All rights reserved.</h4>
</footer>
</body>
</html>