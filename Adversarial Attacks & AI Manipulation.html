<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title data-i18n="page_title">Противникови атаки и манипулация с AI</title>
    <link rel="stylesheet" href="css/style.css" />
    <script src="./js/languageSwitcher.js" defer></script>
  </head>
  <body>
    <header>
      <nav class="navbar">
        <a href="index.html" class="logo">
          <img src="images/logo.png" alt="AI Logo" />
          <span data-i18n="logo_text"
            >Изкуствен интелект в киберсигурността</span
          >
        </a>
        <div class="hamburger" onclick="toggleMenu()">
          <div></div>
          <div></div>
          <div></div>
        </div>
        <ul class="nav-links">
          <li><a href="index.html" data-i18n="nav_home">Начало</a></li>
          <li>
            <a href="App.html" data-i18n="nav_applications">Приложения</a>
          </li>
          <li><a href="Risks.html" data-i18n="nav_risks">Рискове</a></li>
          <li><a href="About_us.html" data-i18n="nav_about">За нас</a></li>
          <li>
            <a
              href="Go_to_chatGPT.html"
              class="cta-button"
              data-i18n="nav_chatgpt"
              >Отиди към chatGPT</a
            >
          </li>
        </ul>
      </nav>
    </header>
    <main class="main-content">
      <img src="images/risks_1.webp" alt="Adversarial Attack Image" />
      <h1 data-i18n="page_title">Противникови атаки и манипулация с AI</h1>
      <p data-i18n="para1">
        Една от най-големите уязвимости в киберсигурността, управлявана от AI, е
        неговата податливост на противникови атаки, при които киберпрестъпниците
        манипулират AI модели, за да избегнат засичане или да нарушат мерките за
        сигурност. За разлика от традиционните заплахи, които се основават на
        експлоатиране на софтуерни уязвимости, противниковите атаки целят самата
        интелигентност на AI системите. Това често се постига чрез отравяне на
        данни, при което атакуващите въвеждат заблуждаващи или злонамерени данни
        в обучителния комплект на AI. Чрез компрометиране на процеса на учене,
        те могат да манипулират AI модели да класифицират заплахите неправилно,
        така че да пренебрегват зловредна активност или дори да я маркират като
        безобидна. По същия начин, атаките за избягване се извършват чрез малки
        корекции в техниките на атака – като модифициране на сигнатурите на
        злонамерен софтуер или поведението при влизане – така че да заобикалят
        AI системите. Този вид експлоатация позволява на киберпрестъпниците да
        избягват засичането на сигурност.
      </p>
      <p data-i18n="para2">
        Разширяването на приложенията на AI в сигурността означава, че хакерите
        проектират все по-усъвършенствани подходи за манипулиране на такива
        системи. Съществуват атаки за избягване, които включват минимални,
        незабележими корекции в злонамерени товари, т.е. препрограмиране на
        злонамерен софтуер по такъв начин, че той вече да не съвпада с моделите
        на заплахите, които обучена AI система разпознава. Има и инверсия на
        модела, при която атакуващите разкриват поведението на AI с цел обръщане
        на процеса на вземане на решения, ефективно разкривайки как се откриват
        заплахите. Според това, хакерите могат да създават нови вектори за
        атака, които напълно остават незабелязани. Фишинг имейли, които преди
        разчитаха на очевидни предупредителни знаци – напр. грешки в правописа
        или подозрителни линкове – сега са модифицирани от AI, за да изглеждат
        по-реалистични и затова по-трудни за засичане от AI-базирани филтри.
      </p>
      <p data-i18n="para3">
        Такъв тип противникова атака трябва да бъде посрещнат с многостепенна
        защитна стратегия, която използва силата на AI, като същевременно
        позволява човешки контрол и непрекъснато усъвършенстване на модела.
        Организациите трябва да прилагат противниково обучение, при което AI
        моделите се подлагат на симулирани атаки, за да станат устойчиви. Освен
        това, използването на обяснима AI (XAI) ще позволи на екипите по
        сигурност да имат по-добър поглед върху процесите на вземане на решения,
        което улеснява откриването на манипулации. Редовните одити на моделите,
        в комбинация с интегрирането на AI с правилно базирани мерки за
        откриване, както и човешки анализ на заплахите, укрепват защитата.
        Докато AI предоставя мощно предимство в киберсигурността, никога няма да
        има недостиг от атакуващи, които търсят начини да го експлоатират –
        затова е от съществено значение организациите да останат една крачка
        пред тях чрез непрекъснат мониторинг, фино настройване и проактивна
        интелигенция за заплахи.
      </p>
    </main>
    <footer class="footer-content">
      <section class="items-wrap">
        <section class="links-wrap">
          <a href="https://github.com/Ivana390/AI-into-cybersecurity">
            <img src="images/git-logo.png" alt="GitHub" />
          </a>
          <a href="https://discord.com/channels/@me">
            <img src="images/discord.png" alt="Discord" />
          </a>
        </section>
        <section class="links-wrap">
          <h3 data-i18n="terms_of_use">УСЛОВИЯ ЗА ПОЛЗВАНЕ</h3>
          <h3 data-i18n="privacy_policy">ПОЛИТИКА ЗА ПОВЕРИТЕЛНОСТ</h3>
          <h3 data-i18n="site_map">КАРТА НА САЙТА</h3>
        </section>
        <section class="links-wrap">
          <img
            src="images/en-logo.png"
            alt="English"
            id="footer-lang-en"
            onclick="switchLanguage('en')"
          />
          <img
            src="images/bg-logo.png"
            alt="Български"
            id="footer-lang-bg"
            onclick="switchLanguage('bg')"
          />
          <img
            src="images/de-logo.png"
            alt="Deutsch"
            id="footer-lang-de"
            onclick="switchLanguage('de')"
          />
        </section>
      </section>
      <h4 data-i18n="copyright">
        Авторско право © 2025 Изкуствен интелект (AI) в киберсигурността. Всички
        права запазени.
      </h4>
    </footer>
    <script>
      const translations = {
        en: {
          page_title: "Adversarial Attacks & AI Manipulation",
          logo_text: "AI into cybersecurity",
          nav_home: "Home",
          nav_applications: "Applications",
          nav_risks: "Risks",
          nav_about: "About us",
          nav_chatgpt: "Go to chatGPT",
          para1:
            "One of the biggest vulnerabilities in AI-driven cybersecurity is its susceptibility to adversarial attacks, where cybercriminals manipulate AI models to evade detection or disrupt security measures. Unlike traditional threats that rely on exploiting software vulnerabilities, adversarial attacks target the very intelligence of AI systems. This is often achieved through data poisoning, where attackers introduce misleading or malicious data into the AI’s training set. By compromising the learning process, they can manipulate AI models into misclassifying threats so that they overlook malicious activity or even mark it as benign. Likewise, model evasion attacks are conducted through minor adjustments in attack techniques—like modifying malware signatures or login behavior—so that they bypass AI systems. This type of exploitation can enable cybercriminals to evade security unnoticed.",
          para2:
            "The growing applications of AI for security mean there are more sophisticated approaches that hackers are designing to tamper with such systems. There are evasion attacks that entail infinitesimal, imperceptible tweaks to nefarious payloads, i.e., recoding malware in a way that it will no longer match patterns of threats a trained AI recognizes. There is model inversion where attackers uncover AI behavior for purposes of reversing the decision process, effectively uncovering how it detects threats. According to this, hackers are able to craft fresh attack vectors that entirely go unnoticed. Phishing emails that used to rely on obvious red flags—e.g., misspellings or dodgy links—are now modified by AI to be more realistic, and hence more challenging for AI-powered filters to detect.",
          para3:
            "This kind of adversarial attack must be met by a multi-layer defense strategy utilizing the strength of AI while, simultaneously, allowing human control and continuous model refinement. Organizations must apply adversarial training, in which AI models undergo mock attacks, in a manner that they become robust. In addition, using explainable AI (XAI) will allow security teams to have better insight into decision-making processes that can make it easier to detect manipulation. Regular model audits, in addition to fusing AI with rule-based detection traditional cybersecurity practices, and human threat analysis make defenses stronger. While AI introduces a powerful advantage to cybersecurity, there is never going to be any shortage of attackers seeking ways to exploit it against us—so it is crucial that organizations stay ahead of them with ongoing monitoring, fine-tuning, and proactive threat intelligence.",
          terms_of_use: "TERMS OF USE",
          privacy_policy: "PRIVACY POLICY",
          site_map: "SITE MAP",
          copyright:
            "Copyright © 2025 Artificial intelligence (AI) into cybersecurity. All rights reserved.",
        },
        bg: {
          page_title: "Противникови атаки и манипулация с AI",
          logo_text: "Изкуствен интелект в киберсигурността",
          nav_home: "Начало",
          nav_applications: "Приложения",
          nav_risks: "Рискове",
          nav_about: "За нас",
          nav_chatgpt: "Отиди към chatGPT",
          para1:
            "Една от най-големите уязвимости в киберсигурността, управлявана от AI, е неговата податливост на противникови атаки, при които киберпрестъпниците манипулират AI модели, за да избегнат засичане или да нарушат мерките за сигурност. За разлика от традиционните заплахи, които се основават на експлоатиране на софтуерни уязвимости, противниковите атаки целят самата интелигентност на AI системите. Това често се постига чрез отравяне на данни, при което атакуващите въвеждат заблуждаващи или злонамерени данни в обучителния комплект на AI. Чрез компрометиране на процеса на учене, те могат да манипулират AI модели да класифицират заплахите неправилно, така че да пренебрегват зловредна активност или дори да я маркират като безобидна. По същия начин, атаките за избягване се извършват чрез малки корекции в техниките на атака – като модифициране на сигнатурите на злонамерен софтуер или поведението при влизане – така че да заобикалят AI системите. Този вид експлоатация позволява на киберпрестъпниците да избягват засичането на сигурност.",
          para2:
            "Разширяването на приложенията на AI в сигурността означава, че хакерите проектират все по-усъвършенствани подходи за манипулиране на такива системи. Съществуват атаки за избягване, които включват минимални, незабележими корекции в злонамерени товари, т.е. препрограмиране на злонамерен софтуер по такъв начин, че той вече да не съвпада с моделите на заплахите, които обучена AI система разпознава. Има и инверсия на модела, при която атакуващите разкриват поведението на AI с цел обръщане на процеса на вземане на решения, ефективно разкривайки как се откриват заплахите. Според това, хакерите могат да създават нови вектори за атака, които напълно остават незабелязани. Фишинг имейли, които преди разчитаха на очевидни предупредителни знаци – напр. грешки в правописа или подозрителни линкове – сега са модифицирани от AI, за да изглеждат по-реалистични и затова по-трудни за засичане от AI-базирани филтри.",
          para3:
            "Такъв тип противникова атака трябва да бъде посрещнат с многостепенна защитна стратегия, която използва силата на AI, като същевременно позволява човешки контрол и непрекъснато усъвършенстване на модела. Организациите трябва да прилагат противниково обучение, при което AI моделите се подлагат на симулирани атаки, за да станат устойчиви. Освен това, използването на обяснима AI (XAI) ще позволи на екипите по сигурност да имат по-добър поглед върху процесите на вземане на решения, което улеснява откриването на манипулации. Редовните одити на моделите, в комбинация с интегрирането на AI с правилно базирани мерки за откриване, както и човешки анализ на заплахите, укрепват защитата. Докато AI предоставя мощно предимство в киберсигурността, никога няма да има недостиг от атакуващи, които търсят начини да го експлоатират – затова е от съществено значение организациите да останат една крачка пред тях чрез непрекъснат мониторинг, фино настройване и проактивна интелигенция за заплахи.",
          terms_of_use: "УСЛОВИЯ ЗА ПОЛЗВАНЕ",
          privacy_policy: "ПОЛИТИКА ЗА ПОВЕРИТЕЛНОСТ",
          site_map: "КАРТА НА САЙТА",
          copyright:
            "Авторско право © 2025 Изкуствен интелект (AI) в киберсигурността. Всички права запазени.",
        },
        de: {
          page_title: "Adversariale Angriffe & KI-Manipulation",
          logo_text: "KI in der Cybersicherheit",
          nav_home: "Startseite",
          nav_applications: "Anwendungen",
          nav_risks: "Risiken",
          nav_about: "Über uns",
          nav_chatgpt: "Zu chatGPT gehen",
          para1:
            "Eine der größten Schwachstellen in der KI-gesteuerten Cybersicherheit ist ihre Anfälligkeit für adversariale Angriffe, bei denen Cyberkriminelle KI-Modelle manipulieren, um einer Erkennung zu entgehen oder Sicherheitsmaßnahmen zu stören. Im Gegensatz zu herkömmlichen Bedrohungen, die auf der Ausnutzung von Softwareschwachstellen beruhen, zielen adversariale Angriffe direkt auf die Intelligenz von KI-Systemen ab. Dies wird oft durch Datenvergiftung erreicht, bei der Angreifer irreführende oder bösartige Daten in den Trainingsdatensatz der KI einspeisen. Durch die Beeinträchtigung des Lernprozesses können sie KI-Modelle so manipulieren, dass sie Bedrohungen falsch klassifizieren, wodurch schädliche Aktivitäten übersehen oder sogar als harmlos eingestuft werden. Ebenso werden Modell-Evasionsangriffe durch geringfügige Anpassungen in den Angriffstechniken durchgeführt – wie das Modifizieren von Malware-Signaturen oder Login-Verhalten – sodass sie an den KI-Systemen vorbeikommen. Diese Art der Ausnutzung ermöglicht es Cyberkriminellen, Sicherheitsmaßnahmen unbemerkt zu umgehen.",
          para2:
            "Die zunehmende Anwendung von KI in der Sicherheit führt dazu, dass Hacker immer ausgefeiltere Methoden entwickeln, um solche Systeme zu manipulieren. Es gibt Evasionsangriffe, bei denen winzige, kaum wahrnehmbare Anpassungen an bösartigen Nutzlasten vorgenommen werden, d.h. Malware wird so umprogrammiert, dass sie nicht mehr mit den Mustern übereinstimmt, die ein trainiertes KI-System erkennt. Es gibt auch Modellinversion, bei der Angreifer das Verhalten der KI aufdecken, um den Entscheidungsprozess umzukehren und so effektiv zu verstehen, wie Bedrohungen erkannt werden. Dadurch können Hacker neue Angriffsvektoren schaffen, die völlig unbemerkt bleiben. Phishing-E-Mails, die früher auf offensichtliche Warnzeichen wie Tippfehler oder verdächtige Links setzten, werden nun von KI modifiziert, um realistischer zu erscheinen und dadurch schwerer von KI-basierten Filtern erkannt zu werden.",
          para3:
            "Solche adversarialen Angriffe müssen mit einer mehrschichtigen Verteidigungsstrategie begegnet werden, die die Stärke der KI nutzt und gleichzeitig menschliche Kontrolle sowie kontinuierliche Modellanpassung ermöglicht. Organisationen sollten adversariales Training anwenden, bei dem KI-Modelle simulierten Angriffen ausgesetzt werden, um robust zu werden. Darüber hinaus ermöglicht der Einsatz von erklärbarer KI (XAI) den Sicherheitsteams, tiefere Einblicke in die Entscheidungsprozesse zu erhalten, was das Erkennen von Manipulationen erleichtert. Regelmäßige Modellüberprüfungen, kombiniert mit der Integration von KI in regelbasierte Sicherheitsmaßnahmen und menschlicher Bedrohungsanalyse, stärken die Abwehr. Während KI einen mächtigen Vorteil in der Cybersicherheit bietet, wird es niemals an Angreifern mangeln, die Wege suchen, sie auszunutzen – daher ist es entscheidend, dass Organisationen durch kontinuierliches Monitoring, Feinabstimmung und proaktive Bedrohungsaufklärung stets einen Schritt voraus bleiben.",
          terms_of_use: "NUTZUNGSBEDINGUNGEN",
          privacy_policy: "DATENSCHUTZRICHTLINIE",
          site_map: "SEITENÜBERICHT",
          copyright:
            "Urheberrecht © 2025 Künstliche Intelligenz (KI) in der Cybersicherheit. Alle Rechte vorbehalten.",
        },
      };

      function switchLanguage(lang) {
        const elements = document.querySelectorAll("[data-i18n]");
        elements.forEach((el) => {
          const key = el.getAttribute("data-i18n");
          if (translations[lang] && translations[lang][key]) {
            el.innerHTML = translations[lang][key];
          }
        });
        document.title = translations[lang].page_title;
      }

      document.addEventListener("DOMContentLoaded", () => {
        к;
        switchLanguage("en");
      });
    </script>
    <script src="./js/toggleMenu.js"></script>
  </body>
</html>
